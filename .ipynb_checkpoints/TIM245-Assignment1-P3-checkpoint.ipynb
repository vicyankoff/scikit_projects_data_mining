{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Data Validation and Pre-Processing (40 points)\n",
    "In this part of the assignment, we'll cover all of the data munging that's a pre-requisite for data mining. First, we'll look at missing values. Next, we'll look at data normalization and feature aggregation. Last, we'll cover some feature selection and dimensionality reduction techniques. For these exercises, we'll use some data from the Cleveland Clinic used to predict heart disesase. You can read more about the dataset [here](http://archive.ics.uci.edu/ml/datasets/Heart+Disease) and even download similar data from three other clinics.\n",
    "To make life a little easier, I'll list the attributes and their types here.\n",
    "* Age: ratio\n",
    "* Sex: nominal\n",
    "* ChestPainType: nominal\n",
    "* RestingBP: ratio\n",
    "* Cholesterol: ratio\n",
    "* FastingBloodSugar: nominal\n",
    "* RestingECG: nominal\n",
    "* MaxHeartRate: ratio\n",
    "* ExerciseInducedAngine: nominal\n",
    "* STExerciseDepression: ratio\n",
    "* STExercisePeakSlope: ordinal\n",
    "* FlouroscopyVessels: ratio\n",
    "* Thalassemia: nominal\n",
    "* Heart Disease: nominal (label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Preliminaries\n",
    "\n",
    "#Show plots in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# To start we import some prerequisites\n",
    "from sklearn import datasets, preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib2\n",
    "\n",
    "#let's load the data\n",
    "heart_data = urllib2.urlopen(\"http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\")\n",
    "heart = pd.read_csv(heart_data, quotechar='\"', skipinitialspace=True, names=['Age', 'Sex', 'ChestPainType', 'RestingBP', 'Cholesterol', 'FastingBloodSugar', 'RestingECG', 'MaxHeartRate', 'ExerciseInducedAngina', 'STExerciseDepression', 'STExercisePeakSlope', 'FlouroscopyVessels', 'Thalassemia', 'HeartDisease'], na_values=\"?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To make things a bit more difficult, let's break the dataset even more. \n",
    "# We'll replace a nominal attribute with an unknown value\n",
    "heart.loc[::10,'Sex']=2\n",
    "# And put some inconsistent values for blood pressure\n",
    "heart.loc[::7,'RestingBP']=-100\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values\n",
    "Real data often has missing or erroneous values. The pandas documentation has a [great writeup](http://pandas.pydata.org/pandas-docs/stable/missing_data.html) on dealing with missing data. In the dataset above, there are some naturally missing values, as well as erroneous values for the patient's sex and resting blood pressure. Let's try to deal with these data issues. First, we'll see how bad the data quality issues are. We'll use some tools to determine the scope of the missing and inconsistent values.\n",
    "\n",
    "First let's look at the size of our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print heart.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However some of these 303 instances may have missing values. Let's see if we can find any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print heart.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there are four missing values for FlouroscopyVessels and two missing values for Thalassemia. That's not too bad, but maybe there are some other data issues to worry about...\n",
    "\n",
    "Let's look at the Sex field - the dataset description states that these values are coded as 0 for female and 1 for male, which means that values other than 0 or 1 are invalid. Are there any such values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(heart[(heart['Sex'] > 1) | (heart['Sex'] < 0)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 31 invalid values in the dataset! Let's flag these inconsistent values by setting them to the value Python uses to indicate erroneous numerical values. That value is called NotaNumber (NaN). Let's set those invalid values to NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "heart.loc[(heart['Sex'] > 1) | (heart['Sex'] < 0),'Sex'] = np.nan\n",
    "\n",
    "#As a sanity check, let's make sure those values were updated\n",
    "print heart[(heart['Sex'] > 1) | (heart['Sex'] < 0)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now let's remove these and any other missing values from the dataset using the dropna() function\n",
    "heart_cleaned = heart.dropna()\n",
    "print heart_cleaned.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Missing Values (20 points)\n",
    "After all that work, you'll still need to fix the `heart_cleaned` data a little bit.\n",
    "\n",
    "1.   The RestingBP attribute also has some data quality issues - sometimes it's negative. Replace these values with NaN\n",
    "2.   Replace the missing values (NaN) with:\n",
    "    * the mean value\n",
    "    * the median value\n",
    "    \n",
    "    In each case, compute descriptive statistics and report whether the effect on mean, median, and standard deviation.\n",
    "\n",
    "3.    Replace the missing values with means/medians computed for each sex (0 or 1) instead of the global mean/median. Compute the descriptive statistics for the dataset. How do the global mean, median and standard deviation change?\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1 \n",
    "print len(heart_cleaned[heart_cleaned['RestingBP'] < 0])\n",
    "heart_cleaned.loc[heart_cleaned['RestingBP'] < 0, 'RestingBP'] = np.nan\n",
    "print len(heart_cleaned[heart_cleaned['RestingBP'] < 0])\n",
    "\n",
    "# 2\n",
    "# print heart_cleaned.mean() # 131.34\n",
    "# heart_cleaned.fillna(heart_cleaned.mean(), inplace=True)\n",
    "# print heart_cleaned.describe()\n",
    "# #              Age         Sex  ChestPainType   RestingBP  Cholesterol  \\\n",
    "# count  266.000000  266.000000     266.000000  266.000000   266.000000   \n",
    "# mean    54.571429    0.680451       3.169173  131.340611   248.304511   \n",
    "# std      9.033093    0.467181       0.954408   16.064426    52.845318   \n",
    "# min     29.000000    0.000000       1.000000   94.000000   126.000000   \n",
    "# 25%     48.000000    0.000000       3.000000  120.000000   212.000000   \n",
    "# 50%     56.000000    1.000000       3.000000  130.670306   243.000000   \n",
    "# 75%     61.000000    1.000000       4.000000  140.000000   277.750000   \n",
    "# max     77.000000    1.000000       4.000000  192.000000   564.000000   \n",
    "\n",
    "#        FastingBloodSugar  RestingECG  MaxHeartRate  ExerciseInducedAngina  \\\n",
    "# count         266.000000  266.000000    266.000000             266.000000   \n",
    "# mean            0.146617    1.000000    149.458647               0.319549   \n",
    "# std             0.354390    0.994324     23.517159               0.467181   \n",
    "# min             0.000000    0.000000     71.000000               0.000000   \n",
    "# 25%             0.000000    0.000000    132.000000               0.000000   \n",
    "# 50%             0.000000    1.000000    154.000000               0.000000   \n",
    "# 75%             0.000000    2.000000    166.000000               1.000000   \n",
    "# max             1.000000    2.000000    202.000000               1.000000   \n",
    "\n",
    "#        STExerciseDepression  STExercisePeakSlope  FlouroscopyVessels  \\\n",
    "# count            266.000000           266.000000          266.000000   \n",
    "# mean               1.075188             1.601504            0.706767   \n",
    "# std                1.187126             0.625735            0.953895   \n",
    "# min                0.000000             1.000000            0.000000   \n",
    "# 25%                0.000000             1.000000            0.000000   \n",
    "# 50%                0.800000             2.000000            0.000000   \n",
    "# 75%                1.750000             2.000000            1.000000   \n",
    "# max                6.200000             3.000000            3.000000   \n",
    "\n",
    "#        Thalassemia  HeartDisease  \n",
    "# count   266.000000    266.000000  \n",
    "# mean      4.703008      0.969925  \n",
    "# std       1.938233      1.234350  \n",
    "# min       3.000000      0.000000  \n",
    "# 25%       3.000000      0.000000  \n",
    "# 50%       3.000000      0.000000  \n",
    "# 75%       7.000000      2.000000  \n",
    "# max       7.000000      4.000000  \n",
    "\n",
    "# The effect is: \n",
    "# mean - stayed the same at 131.34\n",
    "# median - increased from 130 -> 130.670306\n",
    "# std - decreased from 17.318917 -> 16.064426\n",
    "\n",
    "\n",
    "# Part 2 b) Replace the missing values with the median\n",
    "# # print heart_cleaned.median() # 130\n",
    "# heart_cleaned.fillna(heart_cleaned.median(), inplace=True)\n",
    "# print heart_cleaned.describe()\n",
    "#              Age         Sex  ChestPainType   RestingBP  Cholesterol  \\\n",
    "# count  266.000000  266.000000     266.000000  266.000000   266.000000   \n",
    "# mean    54.571429    0.680451       3.169173  131.154135   248.304511   \n",
    "# std      9.033093    0.467181       0.954408   16.071148    52.845318   \n",
    "# min     29.000000    0.000000       1.000000   94.000000   126.000000   \n",
    "# 25%     48.000000    0.000000       3.000000  120.000000   212.000000   \n",
    "# 50%     56.000000    1.000000       3.000000  130.000000   243.000000   \n",
    "# 75%     61.000000    1.000000       4.000000  140.000000   277.750000   \n",
    "# max     77.000000    1.000000       4.000000  192.000000   564.000000   \n",
    "\n",
    "#        FastingBloodSugar  RestingECG  MaxHeartRate  ExerciseInducedAngina  \\\n",
    "# count         266.000000  266.000000    266.000000             266.000000   \n",
    "# mean            0.146617    1.000000    149.458647               0.319549   \n",
    "# std             0.354390    0.994324     23.517159               0.467181   \n",
    "# min             0.000000    0.000000     71.000000               0.000000   \n",
    "# 25%             0.000000    0.000000    132.000000               0.000000   \n",
    "# 50%             0.000000    1.000000    154.000000               0.000000   \n",
    "# 75%             0.000000    2.000000    166.000000               1.000000   \n",
    "# max             1.000000    2.000000    202.000000               1.000000   \n",
    "\n",
    "#        STExerciseDepression  STExercisePeakSlope  FlouroscopyVessels  \\\n",
    "# count            266.000000           266.000000          266.000000   \n",
    "# mean               1.075188             1.601504            0.706767   \n",
    "# std                1.187126             0.625735            0.953895   \n",
    "# min                0.000000             1.000000            0.000000   \n",
    "# 25%                0.000000             1.000000            0.000000   \n",
    "# 50%                0.800000             2.000000            0.000000   \n",
    "# 75%                1.750000             2.000000            1.000000   \n",
    "# max                6.200000             3.000000            3.000000   \n",
    "\n",
    "#        Thalassemia  HeartDisease  \n",
    "# count   266.000000    266.000000  \n",
    "# mean      4.703008      0.969925  \n",
    "# std       1.938233      1.234350  \n",
    "# min       3.000000      0.000000  \n",
    "# 25%       3.000000      0.000000  \n",
    "# 50%       3.000000      0.000000  \n",
    "# 75%       7.000000      2.000000  \n",
    "# max       7.000000      4.000000  \n",
    "\n",
    "# The effect is: \n",
    "# mean - decreased from 131.340611 -> 131.154135\n",
    "# median - stayed the same at 130 \n",
    "# std - decreased from 17.318917 -> 16.071148\n",
    "\n",
    "\n",
    "# 3 \n",
    "heart_cleaned_female = heart_cleaned[heart_cleaned.Sex == 0]\n",
    "heart_cleaned_male = heart_cleaned[heart_cleaned.Sex == 1]\n",
    "\n",
    "# print heart_cleaned_female.mean() # 132.2\n",
    "# print heart_cleaned_male.mean() # 130.96 ~ 131\n",
    "# print heart_cleaned.describe()\n",
    "# heart_cleaned.loc[(heart_cleaned['Sex'] == 0) & (heart_cleaned['RestingBP'].isnull()), 'RestingBP'] = 132.2\n",
    "# heart_cleaned.loc[(heart_cleaned['Sex'] == 1) & (heart_cleaned['RestingBP'].isnull()), 'RestingBP'] = 131\n",
    "# print heart_cleaned.describe()\n",
    "# Descriptive stats: \n",
    "#          RestingBP  \n",
    "# count  266.000000     \n",
    "# mean   131.360902     \n",
    "# std    16.066014      \n",
    "# min    94.000000      \n",
    "# 25%    120.000000      \n",
    "# 50%    130.500000    \n",
    "# 75%    140.000000    \n",
    "# max    192.000000 \n",
    "\n",
    "# # The effect is: \n",
    "# # mean - increased 131.34 -> 131.36\n",
    "# # median - increased from 130 -> 130.5\n",
    "# # std - decreased from 17.32 -> 16.1\n",
    "\n",
    "\n",
    "# b) Replace the missing values with means/medians computed for each sex\n",
    "# # print heart_cleaned_female.median() # 130\n",
    "# # print heart_cleaned_male.median() # 130\n",
    "# print heart_cleaned.describe()\n",
    "heart_cleaned.loc[(heart_cleaned['Sex'] == 0) & (heart_cleaned['RestingBP'].isnull()), 'RestingBP'] = 130\n",
    "heart_cleaned.loc[(heart_cleaned['Sex'] == 1) & (heart_cleaned['RestingBP'].isnull()), 'RestingBP'] = 130\n",
    "print heart_cleaned.describe()\n",
    "# Descriptive stats:\n",
    "#         RestingBP\n",
    "# count   266.000000      \n",
    "# mean    131.154135      \n",
    "# std     16.071148   \n",
    "# min     94.000000 \n",
    "# 25%     120.000000   \n",
    "# 50%     130.000000   \n",
    "# 75%     140.000000\n",
    "# max     192.000000\n",
    "\n",
    "# # The effect is: \n",
    "# # mean - decreased from 131.34 -> 131.15\n",
    "# # median - stayed the same 130 -> 130 \n",
    "# # std - decreased from 17.32 -> 16.07\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming and Generating Features\n",
    "In class, we discussed various ways to transform features. There are simple approaches like normalizing features and more complex approaches that involve generating new features from the ones we already have. Let's look at an example of each.\n",
    "\n",
    "First, let's take the ratio features and normalize them into a function of the data range. To do this, we'll use some tools from scikit-learn. Now would be a great time to familiarize yourself with the [data preprocessing tools](http://scikit-learn.org/stable/modules/preprocessing.html) available in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Let's re-load a clean dataset, so our results don't depend on your answers to Q1\n",
    "heart_data = urllib2.urlopen(\"http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\")\n",
    "heart = pd.read_csv(heart_data, quotechar='\"', skipinitialspace=True, names=['Age', 'Sex', 'ChestPainType', 'RestingBP', 'Cholesterol', 'FastingBloodSugar', 'RestingECG', 'MaxHeartRate', 'ExerciseInducedAngina', 'STExerciseDepression', 'STExercisePeakSlope', 'FlouroscopyVessels', 'Thalassemia', 'HeartDisease'], na_values=\"?\")\n",
    "heart=heart.dropna()\n",
    "\n",
    "#Now let's look at a snippet of the original data\n",
    "heart_ratio =  heart.loc[:,['Age','RestingBP','Cholesterol','MaxHeartRate','STExercisePeakSlope','FlouroscopyVessels']]\n",
    "print heart_ratio.head()\n",
    " \n",
    "#The MinMaxScaler scales each value by subtracting the minimum and then dividing by the range,\n",
    "# or scaled_value = (value - min)/(max-min)\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "#Scale the heart data\n",
    "heart_ratio_scaled_values = scaler.fit_transform(heart_ratio.values);\n",
    "\n",
    "#Put the result back into a dataframe\n",
    "heart_ratio_scaled = pd.DataFrame(heart_ratio_scaled_values, columns = heart_ratio.columns)\n",
    "print heart_ratio_scaled.head()\n",
    "\n",
    "#heart_ratio_scaled = pd.DataFrame(preprocessing.MinMaxScaler().fit_transform(heart_ratio.values), columns = heart_ratio.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing features is a good idea, since some algorithms won't understand that age and cholesterol are equally important when the cholesterol can be an order of magnitude larger than the age. Transforming features to the [0,1] range makes them all equally important. Note that we did not normalize the nominal features, but there would be no harm in doing so since the nominal properties (each value has a distinct identity) would be preserved.\n",
    "\n",
    "Another common task is generating new features from your existing data. Let's look at a simple way of generating new features: polynomial features.\n",
    "\n",
    "In many cases, we may want to look at how one feature interacts with another. Scikit-learn provides a function to automatically generate all of these features, `PolynomialFeatures`. Read the [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures) to understand what it does. We'll apply it to our dataset below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Remove the label from the features\n",
    "heart_unlabeled = heart[['Age', 'Sex', 'ChestPainType', 'RestingBP', 'Cholesterol', 'FastingBloodSugar', 'RestingECG', 'MaxHeartRate', 'ExerciseInducedAngina', 'STExerciseDepression', 'STExercisePeakSlope', 'FlouroscopyVessels', 'Thalassemia']]\n",
    "\n",
    "#Scale the values, then generate polynomial features\n",
    "heart_polynomial = pd.DataFrame(preprocessing.PolynomialFeatures().fit_transform(preprocessing.MinMaxScaler().fit_transform(heart_unlabeled.values)))\n",
    "heart_polynomial_values = heart_polynomial.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Feature Transformations (10 points)\n",
    "\n",
    "1.   The MinMaxScaler is pretty limited. The StandardScaler is a bit more powerful. Apply it to the ratio features in the heart dataset `heart_ratio` above. Compute descriptive statistics for the data. What are the mean and standard deviation for each column?\n",
    "\n",
    "2.   How many polynomial features are generated? Explain why this number of features was generated, giving a breakdown of features of each type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See cell bellow, the formatting was bad in markup langauge so I left it as a comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1\n",
    "better_scaler = preprocessing.StandardScaler()\n",
    "heart_ratio_better_scaler_values = better_scaler.fit_transform(heart_ratio.values)\n",
    "heart_ratio_better_scaled = pd.DataFrame(heart_ratio_better_scaler_values, columns = heart_ratio.columns)\n",
    "print heart_ratio_better_scaled.describe()\n",
    "#                 Age     RestingBP   Cholesterol  MaxHeartRate  \\\n",
    "# count  2.970000e+02  2.970000e+02  2.970000e+02  2.970000e+02   \n",
    "# mean  -1.237319e-16  4.810966e-16 -1.911116e-16  5.143660e-16   \n",
    "# std    1.001688e+00  1.001688e+00  1.001688e+00  1.001688e+00   \n",
    "# min   -2.827176e+00 -2.125634e+00 -2.337704e+00 -3.431849e+00   \n",
    "# 25%   -7.241238e-01 -6.594306e-01 -7.002541e-01 -7.247694e-01   \n",
    "# 50%    1.613719e-01 -9.550637e-02 -8.380217e-02  1.484822e-01   \n",
    "# 75%    7.148067e-01  4.684179e-01  5.519138e-01  7.160957e-01   \n",
    "# max    2.485798e+00  3.851964e+00  6.099981e+00  2.287949e+00   \n",
    "\n",
    "#        STExercisePeakSlope  FlouroscopyVessels  \n",
    "# count         2.970000e+02        2.970000e+02  \n",
    "# mean         -1.278439e-16        6.653862e-17  \n",
    "# std           1.001688e+00        1.001688e+00  \n",
    "# min          -9.765832e-01       -7.219761e-01  \n",
    "# 25%          -9.765832e-01       -7.219761e-01  \n",
    "# 50%           6.437811e-01       -7.219761e-01  \n",
    "# 75%           6.437811e-01        3.448244e-01  \n",
    "# max           2.264145e+00        2.478425e+00  \n",
    "\n",
    "# Mean and Std for each column\n",
    "#                Age     RestingBP   Cholesterol  MaxHeartRate   STExercisePeakSlope  FlouroscopyVessels\n",
    "# mean  -1.237319e-16  4.810966e-16 -1.911116e-16  5.143660e-16   -1.278439e-16       6.653862e-17  \n",
    "# std    1.001688e+00  1.001688e+00  1.001688e+00  1.001688e+00   1.001688e+00        1.001688e+00  \n",
    "\n",
    "# 2\n",
    "# There are 105 polynomial features created\n",
    "# The number is generated using the formula\n",
    "#    f(n) = 1 + 2n + nChoose2\n",
    "# where n is the number of features\n",
    "# the 1 is there by default. \n",
    "# The 2n comes from the fact that each feature including it's root power is included ex [1, a, b, a^2, ab, b^2].\n",
    "# and the last term is all the possible combinations between the n features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "Now that we've generated polynomial features, we may have too much data, some of it not very useful. There are two approaches to trimming this dataset into a more manageable size. \n",
    "\n",
    "The first approach is feature selection, a method to choose the features in the data that are the most useful for analysis. Of course, when feature selection occurs before analysis, the selection approaches can't use the label information and rely on statistical information as a criteria for \"useful\" information. Many of the feature selection approaches in scikit-learn use some sort of model that fits the data to decide which features are most useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Feature Selection (5 points)\n",
    "Read the [documentation on feature selection](http://scikit-learn.org/stable/modules/feature_selection.html#feature-selection) in scikit-learn before completing the exercises.\n",
    "\n",
    "1.   Apply the `VarianceThreshold` feature selection algorithm (with threshold 0.2) to the `heart_polynomial` dataset. How many features remain after feature selection is applied?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. There remain 17 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "heart_polynomial_prunned = VarianceThreshold(threshold=(.2 * (1 - .2))).fit_transform(heart_polynomial)\n",
    "print heart_polynomial_prunned.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "The other approach to reducing the number of features is to find a good way to squash many, many numbers into very few numbers. One very popular method to do this is [principal components analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis). The key idea is to combine the many, many attribute values together in a way that maximizes the differences between instances. The outcome is taking data in a high-dimensional space and projecting it into a low-dimensional space while keeping the points spread out. \n",
    "\n",
    "The [scikit-learn documentation](http://scikit-learn.org/stable/modules/decomposition.html#pca) shows how PCA works on the Iris dataset you were exploring in the previous part.  We'll try using PCA with the heart data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load some modules to help\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#extract labels\n",
    "heart_labels = heart['HeartDisease'].values\n",
    "heart_labels[heart_labels > 0] = 1\n",
    "\n",
    "#perform PCA\n",
    "heart_polynomial_pca = PCA(n_components=3).fit_transform(heart_polynomial_values)\n",
    "\n",
    "#Plot\n",
    "ax = Axes3D(plt.figure(), elev=-150, azim=200)\n",
    "ax.scatter(heart_polynomial_pca[:, 0], heart_polynomial_pca[:, 1], heart_polynomial_pca[:, 2], c=heart_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Dimensionality Reduction (5 points)\n",
    "\n",
    "1.   Repeat PCA using the `heart_unlabeled` data, projecting into three dimensions as in the above example. Does the projection do a better job of separating the points? Does the PCA projection produce a clearer delineation of the patients with normal heart function and those with heart disease?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "a) Does the projection do a better job of separating the points? <br>\n",
    "A: no the poits are much closely packed together\n",
    "    \n",
    "b) Does the PCA projection produce a clearer delineation of the patients with normal heart function\n",
    "and those with heart disease?<br>\n",
    "A: no, the PCA projection of the heart_polynomial data produced a much clearer delineation of the patients\n",
    "    with normal heart function and those with the disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PCA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ef7554f29a1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# perform PCA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mheart_unlabeled_pca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheart_unlabeled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAxes3D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mazim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PCA' is not defined"
     ]
    }
   ],
   "source": [
    "# perform PCA \n",
    "heart_unlabeled_pca = PCA(n_components=3).fit_transform(heart_unlabeled.values)\n",
    "\n",
    "# Plot\n",
    "ax = Axes3D(plt.figure(), elev=-150, azim=200)\n",
    "ax.scatter(heart_unlabeled_pca[:,0], heart_unlabeled_pca[:,1], heart_unlabeled_pca[:,2], c=heart_labels)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
